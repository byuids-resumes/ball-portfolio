[
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "I am a dedicated student at Brigham Young University - Idaho (BYUI), pursuing a major in Data Science. My passion lies in database management, and I aspire to work in this field upon graduation. Equipped with strong skills in SQL and Python, along with some experience in HTML, R, and Tableau, I am ready to leverage my knowledge to tackle real-world data challenges.\n\n\n\nEducation: Student at Brigham Young University - Idaho (BYUI), Data Science Major.\nSkills: Proficient in SQL and Python, with familiarity in HTML, R, and Tableau.\nExtracurricular Activities: Actively involved in the Data Science Society at BYUI.\nRoles: Served as a team member and co-project manager within the society.\nProjects: Contributed to various projects independently and collaboratively:\n\nDeveloped a database management system to track equipment for the local fire department.\nCreated an application for tracking and generating analytical charts.\nDesigned an educational math game for elementary school children.\n\n\n\n\n\n\nDatabase Management System: Implemented a solution to efficiently manage equipment inventory for the local fire department.\nAnalytical Application: Developed an application capable of tracking data and generating insightful charts for analysis.\nEducational Game: Designed an engaging math game aimed at elementary school students to facilitate learning."
  },
  {
    "objectID": "index.html#education-and-experience",
    "href": "index.html#education-and-experience",
    "title": "About Me",
    "section": "",
    "text": "Education: Student at Brigham Young University - Idaho (BYUI), Data Science Major.\nSkills: Proficient in SQL and Python, with familiarity in HTML, R, and Tableau.\nExtracurricular Activities: Actively involved in the Data Science Society at BYUI.\nRoles: Served as a team member and co-project manager within the society.\nProjects: Contributed to various projects independently and collaboratively:\n\nDeveloped a database management system to track equipment for the local fire department.\nCreated an application for tracking and generating analytical charts.\nDesigned an educational math game for elementary school children."
  },
  {
    "objectID": "index.html#portfolio-highlights",
    "href": "index.html#portfolio-highlights",
    "title": "About Me",
    "section": "",
    "text": "Database Management System: Implemented a solution to efficiently manage equipment inventory for the local fire department.\nAnalytical Application: Developed an application capable of tracking data and generating insightful charts for analysis.\nEducational Game: Designed an engaging math game aimed at elementary school students to facilitate learning."
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Projects/project1.html",
    "href": "Projects/project1.html",
    "title": "Client Report - Project 1",
    "section": "",
    "text": "Using the name data along with the year people can find many interesting trends including, how pop culture influences baby names, the popularity of biblical names, and often have good guesses of when someone was born based solely on the name. This data shows interesting trends in baby names over the years, and how they are influenced by different factors. However, it is important to note that an individual can’t come to a complete conclusion without further data.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#elevator-pitch",
    "href": "Projects/project1.html#elevator-pitch",
    "title": "Client Report - Project 1",
    "section": "",
    "text": "Using the name data along with the year people can find many interesting trends including, how pop culture influences baby names, the popularity of biblical names, and often have good guesses of when someone was born based solely on the name. This data shows interesting trends in baby names over the years, and how they are influenced by different factors. However, it is important to note that an individual can’t come to a complete conclusion without further data.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-1",
    "href": "Projects/project1.html#questiontask-1",
    "title": "Client Report - Project 1",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nHow does your name at your birth year compare to its use historically?\nMy name, Bethany was most popular from 1980 to 2000. Before and after that there were only a couple hundred people were given that name. However, at its height about 2,000 babies were given that name. I was born in 2004, right when the popularity of the name was dropping off.\n\n\nShow the code\nimport pandas as pd\nimport plotly.express as px\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\n\n# Read the CSV file from the URL\nurl = \"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\"\nnames_year = pd.read_csv(url)\n\n# Filter for the name 'Bethany'\nbethany_names_year = names_year.query('name == \"Bethany\"')\n\n# Create a line plot\nbethany_plot = px.line(bethany_names_year, x = 'year', y = 'Total',\n                       labels={'Total': 'Number of babies born with the name'},\n                       title='Popularity of the name Bethany over the years')\n\n# Add a vertical line at the year 2004\nbethany_plot.add_vline(x = 2004, line_dash = \"dash\", line_color = \"red\", annotation_text = \"My birth year\",\n                       annotation_position = \"bottom left\")\n\n# Show the plot\nbethany_plot.show()\n\n\n                                                \n\n\nFindings\n-The name Bethany was rarely given to new babies until the 1950s\n-In 1988 the name was at its peek of popularity and was given to 3,294 babies.\n-In 2004 when I was born and given the name Bethany it was dropping in popularity very quickly. Only 1,614 babies were given that name.",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-2",
    "href": "Projects/project1.html#questiontask-2",
    "title": "Client Report - Project 1",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nI would guess based on probability that someone named Brittany would be born between 1985 and 1995, which would mean that as of 2024 they would be between the ages of 39 and 29. Before these years there were only about 500 babies given that name per year and after those years there were only about a thousand babies given that name per year. This may seem like a lot, but between 1985 and 1995 it was more like 30,000 per year.\n\n\nShow the code\nimport pandas as pd\nimport plotly.express as px\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\n\n# Read the CSV file from the URL\nurl = \"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\"\nnames_year = pd.read_csv(url)\n\n# filters by name\nBrittany_names_year = names_year.query('name == \"Brittany\"')\n\n\n# Create a line plot\nBrittany_plot = px.line(Brittany_names_year, x = 'year', y = 'Total',\n                       labels={'Total': 'Number of babies born with the name'},\n                       title='Popularity of the name Brittany over the years')\n\n# shows graph\nBrittany_plot.show()\n\n\n                                                \n\n\nFindings\n-The name Brittany was rarely used only a couple of hundred babies were given that name nationwide before the 1980s.\n-The name Brittany rose very quickly starting in the 1980s and reached its peak in 1990 with 32.5K babies given that name.\n-After the peak in 1990 the popularity dropped just as quickly as it rose, leveling out in about the year 2000.\n-After the popularity dropped it stay a little higher overall before the drop of about 1,000 born with that name on average instead of about 500 before the spick. However, this increase was nowhere near where it was at its height.",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-3",
    "href": "Projects/project1.html#questiontask-3",
    "title": "Client Report - Project 1",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names. What trends do you notice?\nThe most apparent trend is that the popularity of all of these biblical names went down substantially in the 1970s. Another notable trend is that all of these names were most popular in the 1950s. Of these names Mary was the most popular at about 54,000 at its height of popularity, followed shortly by Paul, with about 25,000 at its height. However, both Martha and Peter were never as popular at about 11,000 at their height.\n\n\nShow the code\nimport pandas as pd\nimport plotly.express as px\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\n\n# Read the CSV file from the URL\nurl = \"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\"\nnames_year = pd.read_csv(url)\n\n# Filters the data for the years 1920 to 2000 and for the specific names.\nnames = ['Mary', 'Martha', 'Peter', 'Paul']\nnames_year_filtered = names_year[(names_year['year'] &gt;= 1920) & (names_year['year'] &lt;= 2000) & (names_year['name'].isin(names))]\n\n# makes the graph\nfig = px.line(names_year_filtered, x = 'year', y = 'Total', color = 'name',\n              # adds captions and title\n              labels={'Total': 'Total number of babies born with name'},\n              title='Showing trends of biblical names popularity dropping in the 1970s and other trends (1920-2000)')\n\n# Customize layout\nfig.update_layout(\n    xaxis_title = 'Year',\n    legend_title_text = 'Names',\n)\n\n# Show the graph\nfig.show()\n\n\n                                                \n\n\nFindings\n-These examples of biblical names dropped in popularity significantly from the late 1950s to the early 1970s.\n-In order of popularity over the years these names rank from most to least popular: Mary, Paul, Peter, and Martha.\n-These biblical names were all at the height of their popularity around the 1950s.\n-The name Mary at its height was almost 2x as popular as the next most popular name at the time Paul.\n-After the substantial drop in 1975 all of the names stayed fairly popular at between 4 to 1 thousand names given to new babies a year.",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-4",
    "href": "Projects/project1.html#questiontask-4",
    "title": "Client Report - Project 1",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\nIt is hard to say if the movie had an effect on the name’s popularity. The popularity was already going up when the movie came out, but the overall trend of the name being more popular did stay higher. However, in 1997, the year average age of the children who watched that show when it first came out were having children there first children, it was a little lower than before the movie came out. However, in the early 2000s, it picked up significantly. It is hard to say without more information if the movie caused this spick or if it was something else.\n\n\nShow the code\nimport pandas as pd\nimport plotly.express as px\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\n\n# Read the CSV file from the URL\nurl = \"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\"\ndf = pd.read_csv(url)\n\n# Filter by the name 'Elliott'\nElliott_df = df.query('name == \"Elliott\"')\n\n# Create a line plot\nElliott_plot = px.line(Elliott_df, x = 'year', y = 'Total', labels={'Total': 'Number of babies born with the name'},\n                       title = 'Popularity of the name Elliott over the years in relation to the movie ET')\n\n# Add a vertical line at the year 1982\nElliott_plot.add_vline(x=1982, line_dash = \"dash\", line_color = \"red\", annotation_text = \"Release of E.T.\", \n                       annotation_position = \"bottom right\")\n\n# Show the plot\nElliott_plot.show()\n\n\n                                                \n\n\nFindings\n-The name Elliott’s overall trend is steadily going up from the beginning of the data to the end.\n-The name rose in popularity significantly starting in the early 2000s, and as of the data we have could continue to go up.\n-The popularity jumped when the movie came out, however, it slumped again until about 20 years later, so it is hard to say if it was the movie or some other factor that caused the significant spike in the 2000s.\n-The findings of iff the movie had an effect are somewhat inconclusive. We need more data to get a better idea of if the spick was caused by the movie or simply loosely correlated.",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project2.html",
    "href": "Projects/project2.html",
    "title": "Client Report - Project 2",
    "section": "",
    "text": "This data explores flight delays. It compares which airports have the best and worst delays. It also explores the relationship between delays and time of year, along with the percentage of delays that are weather-related.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_json(\"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\")",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#elevator-pitch",
    "href": "Projects/project2.html#elevator-pitch",
    "title": "Client Report - Project 2",
    "section": "",
    "text": "This data explores flight delays. It compares which airports have the best and worst delays. It also explores the relationship between delays and time of year, along with the percentage of delays that are weather-related.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_json(\"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\")",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-1",
    "href": "Projects/project2.html#questiontask-1",
    "title": "Client Report - Project 2",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”). In your report include one record example (one row) from your new data, in the raw JSON format. Your example should display the “NaN” for at least one missing value.\nThe data was corrected, per the specifications of the task.\n\n\nShow the code\n#replaces missing or \"bad\" data with \"good\" data.\n\n#sets replacment dictionary.\nlookup = {\n    '': 'NaN',\n    -999: 'NaN',\n    '\\+': ''  \n}\ndf2 = df.replace(lookup)\n\ndf2 = df.replace(lookup, regex=True)\n\n#shows examples of \"fixed\" data.\ndf2.head(3)\n\n\n\n\n\n\n\n\n\n\nairport_code\nairport_name\nmonth\nyear\nnum_of_flights_total\nnum_of_delays_carrier\nnum_of_delays_late_aircraft\nnum_of_delays_nas\nnum_of_delays_security\nnum_of_delays_weather\nnum_of_delays_total\nminutes_delayed_carrier\nminutes_delayed_late_aircraft\nminutes_delayed_nas\nminutes_delayed_security\nminutes_delayed_weather\nminutes_delayed_total\n\n\n\n\n0\nATL\nAtlanta, GA: Hartsfield-Jackson Atlanta Intern...\nJanuary\n2005.0\n35048\n1500\nNaN\n4598\n10\n448\n8355\n116423.0\n104415\n207467.0\n297\n36931\n465533\n\n\n1\nDEN\nDenver, CO: Denver International\nJanuary\n2005.0\n12687\n1041\n928\n935\n11\n233\n3153\n53537.0\n70301\n36817.0\n363\n21779\n182797\n\n\n2\nIAD\nNaN\nJanuary\n2005.0\n12381\n414\n1058\n895\n4\n61\n2430\nNaN\n70919\n35660.0\n208\n4497\n134881\n\n\n\n\n\n\n\n\nSummary\n-Data that was not present was replaced with NaN as shown on line 3, column 2 (airport_name).\n-Data that was reprecented by -999, was replaced with NaN as shown on line 1, column 7 (num_of_delays).",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-2",
    "href": "Projects/project2.html#questiontask-2",
    "title": "Client Report - Project 2",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nWhich airport has the worst delays? Discuss the metric you chose, and why you chose it to determine the “worst” airport. Your answer should include a summary table that lists (for each airport) the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours.\nFrom my analysis, the Chicago airport has the worst delays. It on average has the worst delays as far as average minutes delayed. It is about second when it is fairly comparable with the Atlanta airport when you compare the total number of flights to the number of delays no matter how long. Adding these two variables together clearly shows that the Chicago airport is the worst when it comes to delays.\n\n\nShow the code\n# mean of num_of_delays_total: 3437\n# mean of num_of_flights_total 16607\n# sets \"bad\" data to the average value in that catigory.\ndf['num_of_delays_total'] = df['num_of_delays_total'].where(df['num_of_delays_total'] != -999, '3437')\ndf['num_of_flights_total'] = df['num_of_flights_total'].where(df['num_of_flights_total'] != -990, '16607')\n\n# shortens ariport names for readablitly\ndf['airport_name'] = df['airport_name'].str.split(':').str[0]\n\n\n# makes chart that shows the relationship between total flights and delayed flights.\nfig = px.scatter(\n    df,\n    x = 'num_of_delays_total',\n    y = 'num_of_flights_total',\n    labels = {'value': 'Metrics'},\n    title = 'Airport Delays Summary',\n    color = 'airport_name'\n)\n\nfig.show()\n\navg_delay_time_by_airport = df.groupby('airport_name')['minutes_delayed_late_aircraft'].mean().reset_index()\n\n# Convert average delay time from minutes to hours\navg_delay_time_by_airport['avg_delay_time_hours'] = avg_delay_time_by_airport['minutes_delayed_late_aircraft'] / 60\n\n# Create a scatter plot\nfig = px.scatter(\n    avg_delay_time_by_airport,\n    x='airport_name',\n    y='avg_delay_time_hours',\n    labels={'avg_delay_time_hours': 'Average Delay Time (hours)'},\n    title='Average Delay Time at Airports',\n    color = 'airport_name'\n)\n\nfig.show()\n\n\n                                                \n\n\n                                                \n\n\nFindings\n-The Chicago airport has the worst delays overall.\n-The Atlanta airport isn’t very far behind the Chachago airport as far as bad delays.\n-The Washington DC temple has the best delays overall.\nFor Referance\nIn the charts the airport names were shorten for clearity. Below are the full names for referance.\n-Atlanta, GA: Hartsfield-Jackson Alanta Inernaional\n-Chicago, IL: Chiccago O’Hare Inernational\n-Denver, CO: Denver Inernaional\n-Salt Lake Ciy, UT: Salt Lake Ciy Inernaional\n-San Diego, CA: San Diego Inernaional\n-San Francisco, CA: San Francisco Inernaional\n-Washington, DC: Washington Dulles International",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-3",
    "href": "Projects/project2.html#questiontask-3",
    "title": "Client Report - Project 2",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nWhat is the best month to fly if you want to avoid delays of any length? Discuss the metric you chose and why you chose it to calculate your answer. Include one chart to help support your answer, with the x-axis ordered by month. (To answer this question, you will need to remove any rows that are missing the Month variable.)\nThe best month to travel if you want no delays is November, closely followed by September. To figure this out I found the average number of carrier delays compared it to each month. Finding the average for each month I found makes it much clearer and easier to see the full picture of what is happening. This also helps with reader bias when they see outliers.\n\n\nShow the code\n# Replace 'n/a' with NaN\ndf['month'].replace('n/a', pd.NA, inplace=True)\n\n# Drop rows with NaN in the 'month' column\ndf.dropna(subset=['month'], inplace=True)\n\ndf['num_of_delays_carrier'] = df['num_of_delays_carrier'].str.replace(r'\\D', '', regex=True)\n\n# convert the 'num_of_delays_carrier' column to integers\ndf['num_of_delays_carrier'] = df['num_of_delays_carrier'].astype(int)\n\navg_delay_per_month = df.groupby('month')['num_of_delays_carrier'].mean().reset_index()\n\n# Define the order of months (ascending)\nmonth_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n\n# Convert 'month' column to categorical with specified order\navg_delay_per_month['month'] = pd.Categorical(avg_delay_per_month['month'], categories=month_order, ordered=True)\n\n# Sort DataFrame by the categorical 'month' column\navg_delay_per_month = avg_delay_per_month.sort_values(by='month')\n\nfig = px.line(avg_delay_per_month,\n              x='month',\n              y='num_of_delays_carrier',\n              title='Average Number of Carrier Delays by Month',\n              labels={'num_of_delays_carrier': 'Average Number of Carrier Delays', 'month': 'Month'})\n\nfig.show()\n\n\n                                                \n\n\nFindings\n-November is the best month if you want to avoid all delays.\n-September is the second-best month if you want to avoid all delays.\n-The worst months for delays are June, July, and December.",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-4",
    "href": "Projects/project2.html#questiontask-4",
    "title": "Client Report - Project 2",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nAccording to the BTS website, the “Weather” category only accounts for severe weather delays. Mild weather delays are not counted in the “Weather” category, but are actually included in both the “NAS” and “Late-Arriving Aircraft” categories. Your job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild). You will need to replace all the missing values in the Late Aircraft variable with the mean. Show your work by printing the first 5 rows of data in a table. Use these three rules for your calculations:__\n100% of delayed flights in the Weather category are due to weather\n\n30% of all delayed flights in the Late-Arriving category are due to weather.\n\nFrom April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%.\nAll data has been corrected according to the desired rules given.\n\n\nShow the code\n#cleans up data\nlookup = {\n    '':'NaN',\n    -999:'NaN',\n    '+':''\n}\ndf2 = df.replace(lookup)\n\n\n#Make new column\ndf['weather_delayed_flights'] = 0\n\n\nlate_aircraft_mean = df['minutes_delayed_late_aircraft'].mean()\n\ndf['minutes_delayed_late_aircraft'].fillna(late_aircraft_mean, inplace=True)\n\ndf['weather_delayed_flights'] += df['num_of_delays_weather']\ndf['weather_delayed_flights'] += df['num_of_delays_late_aircraft'] * 0.3\n\n# print(df['weather_delayed_flights'].head())#???????\ndf2.head(5)\n\n\n\n\n\n\n\n\n\n\nairport_code\nairport_name\nmonth\nyear\nnum_of_flights_total\nnum_of_delays_carrier\nnum_of_delays_late_aircraft\nnum_of_delays_nas\nnum_of_delays_security\nnum_of_delays_weather\nnum_of_delays_total\nminutes_delayed_carrier\nminutes_delayed_late_aircraft\nminutes_delayed_nas\nminutes_delayed_security\nminutes_delayed_weather\nminutes_delayed_total\n\n\n\n\n0\nATL\nAtlanta, GA\nJanuary\n2005.0\n35048\n1500\nNaN\n4598\n10\n448\n8355\n116423.0\n104415\n207467.0\n297\n36931\n465533\n\n\n1\nDEN\nDenver, CO\nJanuary\n2005.0\n12687\n1041\n928\n935\n11\n233\n3153\n53537.0\n70301\n36817.0\n363\n21779\n182797\n\n\n2\nIAD\nNaN\nJanuary\n2005.0\n12381\n414\n1058\n895\n4\n61\n2430\nNaN\n70919\n35660.0\n208\n4497\n134881\n\n\n3\nORD\nChicago, IL\nJanuary\n2005.0\n28194\n1197\n2255\n5415\n5\n306\n9178\n88691.0\n160811\n364382.0\n151\n24859\n638894\n\n\n4\nSAN\nSan Diego, CA\nJanuary\n2005.0\n7283\n572\n680\n638\n7\n56\n1952\n27436.0\n38445\n21127.0\n218\n4326\n91552\n\n\n\n\n\n\n\n\nSummary\nAll data has been corrected according to the desired rules given.",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-5",
    "href": "Projects/project2.html#questiontask-5",
    "title": "Client Report - Project 2",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Discuss what you learn from this graph.\nThis graph shows that overall, not many of the flights are delayed by weather. The highest percentage is SFO at a little under 0.03% of their flights are delayed by weather, and ATL as low as 0.018%. Overall weather doesn’t effect plane travel in the grand scheme of things.\n\n\nShow the code\nlookup = {\n    '':'NaN',\n    -999:'NaN',\n    '+':''\n}\ndf2 = df.replace(lookup)\n\ndf['weather_delayed_flights'] = 0\n\n\nlate_aircraft_mean = df['minutes_delayed_late_aircraft'].mean()\n\ndf['minutes_delayed_late_aircraft'].fillna(late_aircraft_mean, inplace=True)\n\ndf['weather_delayed_flights'] += df['num_of_delays_weather']\ndf['weather_delayed_flights'] += df['num_of_delays_late_aircraft'] * 0.3\n\n# print(df['weather_delayed_flights'].head())\n\n\n\n# Calculate total weather delayed flights and total flights per airport\ntotal_weather_delayed_flights = df.groupby('airport_code')['weather_delayed_flights'].sum()\ntotal_flights_per_airport = df.groupby('airport_code')['num_of_flights_total'].sum()\n\n# Calculate proportion of flights delayed by weather at each airport\nproportion_weather_delayed = total_weather_delayed_flights / total_flights_per_airport\n\n# Sort the DataFrame by values in descending order\nproportion_weather_delayed_sorted = proportion_weather_delayed.sort_values(ascending=False)\n\n# Create bar plot\nfig = px.bar(\n    x=proportion_weather_delayed_sorted.index,\n    y=proportion_weather_delayed_sorted.values,\n    title='Proportion of Flights Delayed by Weather at Each Airport',\n    labels={'x': 'Airport Code', 'y': 'Percentage of Flights Delayed by Weather'}\n)\n\nfig.show()\n\n\n                                                \n\n\nFindings\n-SFO is most affected by weather delays\n-ATL is least affected by weather delays\n-Overall a very low percentage of overall flights are affected by weather delays.\nFor Referance\n-SFO: San Francisco, CA: San Francisco International\n-IAD: Washington, DC: Washington Dulles International\n-SAN: San Diego, CA: San Diego Inernaional\n-ORD: Chicago, IL: Chiccago O’Hare Inernational\n-DEN: Denver, CO: Denver Inernaional\n-SLC: Salt Lake Ciy, UT: Salt Lake Ciy Inernaional\n-ATL: Atlanta, GA: Hartsfield-Jackson Alanta Inernaional",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Templates/DS250_Template.html",
    "href": "Templates/DS250_Template.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Uncomment the entire section to use this template\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Templates/DS350_Template.html",
    "href": "Templates/DS350_Template.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "TODO: Update with template from Paul\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Projects/project4.html",
    "href": "Projects/project4.html",
    "title": "Project 4: Can you predict that?",
    "section": "",
    "text": "This shows housing data and the best ways to find out if a house was built before 1980. It also demests mechine learning, how it works in this context, etc. Overall it demosts a model for mechine learning on how to tell if a house was built before 1980 based on a dataset.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#elevator-pitch",
    "href": "Projects/project4.html#elevator-pitch",
    "title": "Project 4: Can you predict that?",
    "section": "",
    "text": "This shows housing data and the best ways to find out if a house was built before 1980. It also demests mechine learning, how it works in this context, etc. Overall it demosts a model for mechine learning on how to tell if a house was built before 1980 based on a dataset.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-1",
    "href": "Projects/project4.html#questiontask-1",
    "title": "Project 4: Can you predict that?",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.\nOnly one of these charts was very useful at helping find if the house was built before 1980, and that was the ‘Year Built’ chart, witch makes seance. Some of the other charts could help but wouldn’t be that accrate.\n\n\nShow the code\n#chart one sell price \nfig = px.scatter(df, x='before1980', y='sprice', title='Sell Price vs. Year Built', color='before1980', \n             color_discrete_map={1: 'blue', 0: 'red'})\nfig.update_layout(xaxis_title='Year Built (1: Before 1980, 0: After 1980)', yaxis_title='Sell Price')\n\n#year build v before1980\nfig = px.scatter(df, x='before1980', y='yrbuilt', title='Year built vs. before1980')\nfig.update_layout(xaxis_title='Before 1980 (1: Yes, 0: No)', yaxis_title='Year Built')\nfig.show()\n\n# liveable area v before1980\nfig = px.scatter(df, x = 'before1980', y = 'livearea', title = 'Square footage that is liveable v. before1980')\nfig.update_layout(xaxis_title = 'Built Before 1980 (1: Yes, 0: No)', yaxis_title = 'Square footage that is liveable')\nfig.show()\n\n#deduction in price v yearbuilt\nfig = px.scatter(df, x = 'before1980', y = 'deduct', title = 'Deduction from the selling price v. before1980')\nfig.update_layout(xaxis_title = 'Built Before 1980 (1: Yes, 0: No)', yaxis_title = 'Deduction from the selling price')\nfig.show()\n\n\n                                                \n\n\n                                                \n\n\n                                                \n\n\nSummery/Findings\n\nThe year the house is build is the best way out of the four that I tested to find out if a house was built before 1980. (go figure)\n‘Deduction from the selling price’ was okay at finding if a house was built before 1980, but very flewed, it showed that if it was built before 1980 the price was capted at about 20K but after that it capted at 100K, but there is still a lot of overlap.\n‘Square footage that is liveable’ is very useless. Both the built before 1980s and after are almost identical.\nThe ‘Sell Price’ is almost identical to the ‘Deduction from the selling price’, but a little worst, with less above the before 1980s cut-off, witch makes it less usful.",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-2",
    "href": "Projects/project4.html#questiontask-2",
    "title": "Project 4: Can you predict that?",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\nI chose to use the RandomForestClassifier for my model. I used it becouse out of the twentey or so models I tried it had the highest accuracy, without overfiting. I also chhose to use a 30/70 for my perameter, this seemed to work the best and is very commonly recoginized a being genreally a good ratio in the inductry.\n\n\nShow the code\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\n# Remove year built, and before 1980 from data to be trained.\n\n# number bedrooms and number of bathrooms\n\n# corr()\n# df = df.drop('parcel', axis = 1)\n# \n\n\n\n# Read the CSV file into a DataFrame\ndf = pd.read_csv('dwellings_ml.csv')\n\n\n# Remove the '-' characters from the 'parcel' column\ndf['parcel'] = df['parcel'].str.replace('-', '')\n\n\n# Convert the 'parcel' column to integer\ndf['parcel'] = df['parcel'].astype(int)\n\ndf = df.drop('yrbuilt', axis = 1)\n\ndf.columns\n\n# Define features and target\nfeatures = df.drop('before1980', axis=1)  # Drop the target column\ntarget = df['before1980']\n\n# Split data into training and testing sets\ntrain_data, test_data, train_targets, test_targets = train_test_split(features, target, test_size = .3)\n                                                                      \n\n# Instantiate Gaussian Naive Bayes classifier\nclassifier = RandomForestClassifier()\n# classifier = DecisionTreeClassifier()\n# classifier = GradientBoostingClassifier()\n# classifier = tree(max_depth=10, n_estimators=500)\n\n# Train the classifier\nclassifier.fit(train_data, train_targets)\n\n# Predict using the trained classifier\ntargets_predicted = classifier.predict(test_data)\n\n# Calculate accuracy\naccuracy = metrics.accuracy_score(test_targets, targets_predicted)\nprint(\"Accuracy:\", accuracy)\n\n\nAccuracy: 0.9470468431771895\n\n\nSummery/Findings\n\nthis model is 93-95% accurate\nIt uses the GradientBoostingClassifier formula\nIt seporates it 30%-70% training and checking",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-3",
    "href": "Projects/project4.html#questiontask-3",
    "title": "Project 4: Can you predict that?",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.\nAcording to my caculations the top five features are as follows:\n\nParcel\narcstyle_TWO-STORY\nquality_C\nabstrprd\ngartype_Att\n\n\n\nShow the code\n#%%\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#%%\n# Remove year built, and before 1980 from data to be trained.\n\n# number bedrooms and number of bathrooms\n\n# corr()\n# df = df.drop('parcel', axis = 1)\n# \n\n\n#%%\n# Read the CSV file into a DataFrame\ndf = pd.read_csv('dwellings_ml.csv')\n\n#%%\n# Remove the '-' characters from the 'parcel' column\ndf['parcel'] = df['parcel'].str.replace('-', '')\n\n#%%\n# Convert the 'parcel' column to integer\ndf['parcel'] = df['parcel'].astype(int)\n\n\n\n#%%\n\ndf = df.drop('yrbuilt', axis = 1)\n\ndf.columns\n\n#%%\n# Define features and target\nfeatures = df.drop('before1980', axis=1)  # Drop the target column\ntarget = df['before1980']\n\n# Split data into training and testing sets\ntrain_data, test_data, train_targets, test_targets = train_test_split(features, target, test_size = .3)\n                                                                      \n\n# Instantiate Gaussian Naive Bayes classifier\nclassifier = GradientBoostingClassifier()\n# classifier = tree(max_depth=10, n_estimators=500)\n\n# Train the classifier\nclassifier.fit(train_data, train_targets)\n\n# Predict using the trained classifier\ntargets_predicted = classifier.predict(test_data)\n\n# Calculate accuracy\naccuracy = metrics.accuracy_score(test_targets, targets_predicted)\nprint(\"Accuracy:\", accuracy)\n\n\n\n#%%\n\nimport matplotlib.pyplot as plt\n\n# Get feature importances\nfeature_importances = classifier.feature_importances_\n\n# Get the names of the features\nfeature_names = features.columns\n\n# Create a DataFrame to hold feature names and their importances\nfeature_importance_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': feature_importances\n})\n\n# Sort the DataFrame by importance\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\n# Plotting the feature importance\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importance')\nplt.show()\n\n# Display top 5 features\nprint(\"Top 5 features:\")\nprint(feature_importance_df.head())\n\n\nAccuracy: 0.9282804771603143\nTop 5 features:\n               Feature  Importance\n0               parcel    0.265625\n38  arcstyle_ONE-STORY    0.239251\n23           quality_C    0.153528\n26         gartype_Att    0.062257\n1             abstrprd    0.060159",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-4",
    "href": "Projects/project4.html#questiontask-4",
    "title": "Project 4: Can you predict that?",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\nMy metrics are as follows:\nAccuracy: Accuracy measures the proportion of correctly classified instances out of the total instances. It is calculated as the ratio of the number of correct predictions to the total number of predictions. Interpretation: An accuracy of 90% means that 90% of the predictions made by the model are correct. However, accuracy alone might not provide a complete picture of the model’s performance, especially in imbalanced datasets where one class dominates the other.\n- This model is very accurate at about 93-95%%.\nPrecision and Recall: Precision and recall are metrics often used together, especially in binary classification tasks.\nPrecision: Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. It focuses on the accuracy of positive predictions.\nPrecision=TPTP+FPPrecision=TP+FPTP​\n\nRecall: Recall, also known as sensitivity or true positive rate, is the ratio of correctly predicted positive observations to the all observations in actual class.\nRecall=TPTP+FNRecall=TP+FNTP​\n\nInterpretation: Precision indicates the proportion of correctly predicted positive cases out of all cases predicted as positive. Recall indicates the proportion of correctly predicted positive cases out of all actual positive cases. A high precision value means that when the model predicts a positive case, it is likely to be correct. A high recall value means that the model is able to correctly identify most of the positive cases.\n\n- It is very good in the case of this model.",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project3.html",
    "href": "Projects/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project5.html",
    "href": "Projects/project5.html",
    "title": "Client Report - Project 5: The war with Star Wars",
    "section": "",
    "text": "This data set shows trends in starwars fans preferances and demographics about them. This parge mostly shows the porsess of cleaning the data. However it also shows recrations of charts and mechine learning exploring states on this data set.\n\n\nRead and format project data\n# db_file = (\"https://github.com/fivethirtyeight/data/tree/master/star-wars-survey\")\n\n# Read the cleaned data with the correct encoding\ndata = pd.read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv', encoding='latin-1')\n\n# df = pd.read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv', encoding = \"latin-1\", header=[0, 1] )\n# data = df.copy()\n# data.columns = data.columns.map('|'.join).str.strip()\n# list(data.columns)\n# data = pd.read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv\", encoding=\"latin-1\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#elevator-pitch",
    "href": "Projects/project5.html#elevator-pitch",
    "title": "Client Report - Project 5: The war with Star Wars",
    "section": "",
    "text": "This data set shows trends in starwars fans preferances and demographics about them. This parge mostly shows the porsess of cleaning the data. However it also shows recrations of charts and mechine learning exploring states on this data set.\n\n\nRead and format project data\n# db_file = (\"https://github.com/fivethirtyeight/data/tree/master/star-wars-survey\")\n\n# Read the cleaned data with the correct encoding\ndata = pd.read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv', encoding='latin-1')\n\n# df = pd.read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv', encoding = \"latin-1\", header=[0, 1] )\n# data = df.copy()\n# data.columns = data.columns.map('|'.join).str.strip()\n# list(data.columns)\n# data = pd.read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv\", encoding=\"latin-1\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-1",
    "href": "Projects/project5.html#questiontask-1",
    "title": "Client Report - Project 5: The war with Star Wars",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nShorten the column names and clean them up for easier use with pandas. Provide a table or list that exemplifies how you fixed the names.\nI cleaned the data so that it is much easier to understand and makes more seance. This included renaming all columns, convverting yeses to 1 and nos to 0 and many more little things to make it cleaner and earier for pandas.\n\n\nShow the code\n#%%\n\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\n\n\n# Read the data\n# data = pd.read_csv('star_wars_data.csv', encoding='latin-1')\n\n#%%\ncleaned_columns_names = {\n    \"RespondentID\": \"RespondentID\",\n    \"Have you seen any of the 6 films in the Star Wars franchise?\": \"Seen_any_SW\",\n    \"Do you consider yourself to be a fan of the Star Wars film franchise?\": \"SW_fan\",\n\n\n    'Which of the following Star Wars films have you seen? Please select all that apply.': 'Episode_1_seen',\n    'Unnamed: 4': 'Episode_2_seen',\n    'Unnamed: 5': 'Episode_3_seen',\n    'Unnamed: 6': 'Episode_4_seen',\n    'Unnamed: 7': 'Episode_5_seen',\n    'Unnamed: 8': 'Episode_6_seen',\n\n    \"Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.\": \"Episode_1_rank\",\n    'Unnamed: 10': 'Episode_2_rank',\n    'Unnamed: 11': 'Episode_3_rank',\n    'Unnamed: 12': 'Episode_4_rank',\n    'Unnamed: 13': 'Episode_5_rank',\n    'Unnamed: 14': 'Episode_6_rank',\n\n    'Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her.': 'han_solo',\n    'Unnamed: 16': 'luke_skywalker',\n    'Unnamed: 17': 'princess_leia_organa',\n    'Unnamed: 18': 'anakin_skywalker',\n    'Unnamed: 19': 'obi_wan_kenobi',\n    'Unnamed: 20': 'emperor_palpatine',\n    'Unnamed: 21': 'darth_vader',\n    'Unnamed: 22': 'lando_calrissian', \n    'Unnamed: 23': 'boba_fett',\n    'Unnamed: 24': 'c-3p0', \n    'Unnamed: 25': 'r2_d2', \n    'Unnamed: 26': 'jar_jar_binks', \n    'Unnamed: 27': 'padme_amidala',\n    'Unnamed: 28': 'yoda', \n\n    'Which character shot first?': '1st_shot',\n    'Are you familiar with the Expanded Universe?': 'familiar_exp_universe',\n    'Do you consider yourself to be a fan of the Expanded Universe?æ': 'fan_exp_universe',\n    'Do you consider yourself to be a fan of the Star Trek franchise?': 'fan_star_trek',\n    'Gender': 'gender', \n    'Age': 'Age', \n    'Household Income': 'income', \n    'Education': 'education',\n    'Location (Census Region)': 'location',\n}\n\n#%%\n\ndata.rename(columns = cleaned_columns_names, inplace=True)\nprint(data.columns)\n\n\n#%%\n\ncolumns_to_transform = ['Episode_1_seen', 'Episode_2_seen', 'Episode_3_seen', 'Episode_4_seen', 'Episode_5_seen', 'Episode_6_seen']\n\ndata[columns_to_transform] = data[columns_to_transform].fillna(0)\ndata[columns_to_transform] = data[columns_to_transform].applymap(lambda x: 1 if x != 0 else 0)\n\n\n#%%\nYN_dic = {'Yes': 1, 'No': 0}\n\nrank = {'Neither favorably nor unfavorably (neutral)': 1,\n        'Very unfavorably': 2,\n        'Somewhat unfavorably': 3,\n        'Somewhat favorably': 4,\n        'Very favorably': 5,\n        'Unfamiliar (N/A)': 6}\n\n\n#%%\nyes_no_columns = ['Seen_any_SW', 'SW_fan', 'familiar_exp_universe', 'fan_exp_universe', 'fan_star_trek']\n\ndata[yes_no_columns] = data[yes_no_columns].replace(YN_dic)\n\nrank_columns = ['han_solo', 'luke_skywalker', 'princess_leia_organa', 'anakin_skywalker', 'obi_wan_kenobi', 'emperor_palpatine', 'darth_vader', 'lando_calrissian', 'boba_fett', 'c-3p0', 'r2_d2', 'jar_jar_binks', 'padme_amidala', 'yoda']\n\ndata[rank_columns] = data[rank_columns].replace(rank)\n\n\n#%%\n\nprint(data.columns)\nprint(data.head())\n\n\n#%%\n\ndata.to_csv(\"cleaned_data.csv\", index=False)\n\n\nIndex(['RespondentID', 'Seen_any_SW', 'SW_fan', 'Episode_1_seen',\n       'Episode_2_seen', 'Episode_3_seen', 'Episode_4_seen', 'Episode_5_seen',\n       'Episode_6_seen', 'Episode_1_rank', 'Episode_2_rank', 'Episode_3_rank',\n       'Episode_4_rank', 'Episode_5_rank', 'Episode_6_rank', 'han_solo',\n       'luke_skywalker', 'princess_leia_organa', 'anakin_skywalker',\n       'obi_wan_kenobi', 'emperor_palpatine', 'darth_vader',\n       'lando_calrissian', 'boba_fett', 'c-3p0', 'r2_d2', 'jar_jar_binks',\n       'padme_amidala', 'yoda', '1st_shot', 'familiar_exp_universe',\n       'fan_exp_universe', 'fan_star_trek', 'gender', 'Age', 'income',\n       'education', 'location'],\n      dtype='object')\nIndex(['RespondentID', 'Seen_any_SW', 'SW_fan', 'Episode_1_seen',\n       'Episode_2_seen', 'Episode_3_seen', 'Episode_4_seen', 'Episode_5_seen',\n       'Episode_6_seen', 'Episode_1_rank', 'Episode_2_rank', 'Episode_3_rank',\n       'Episode_4_rank', 'Episode_5_rank', 'Episode_6_rank', 'han_solo',\n       'luke_skywalker', 'princess_leia_organa', 'anakin_skywalker',\n       'obi_wan_kenobi', 'emperor_palpatine', 'darth_vader',\n       'lando_calrissian', 'boba_fett', 'c-3p0', 'r2_d2', 'jar_jar_binks',\n       'padme_amidala', 'yoda', '1st_shot', 'familiar_exp_universe',\n       'fan_exp_universe', 'fan_star_trek', 'gender', 'Age', 'income',\n       'education', 'location'],\n      dtype='object')\n   RespondentID Seen_any_SW    SW_fan  Episode_1_seen  Episode_2_seen  \\\n0           NaN    Response  Response               1               1   \n1  3.292880e+09           1         1               1               1   \n2  3.292880e+09           0       NaN               0               0   \n3  3.292765e+09           1         0               1               1   \n4  3.292763e+09           1         1               1               1   \n\n   Episode_3_seen  Episode_4_seen  Episode_5_seen  Episode_6_seen  \\\n0               1               1               1               1   \n1               1               1               1               1   \n2               0               0               0               0   \n3               1               0               0               0   \n4               1               1               1               1   \n\n                             Episode_1_rank  ...  yoda  \\\n0  Star Wars: Episode I  The Phantom Menace  ...  Yoda   \n1                                         3  ...     5   \n2                                       NaN  ...   NaN   \n3                                         1  ...     6   \n4                                         5  ...     5   \n\n                           1st_shot familiar_exp_universe fan_exp_universe  \\\n0                          Response              Response         Response   \n1  I don't understand this question                     1                0   \n2                               NaN                   NaN              NaN   \n3  I don't understand this question                     0              NaN   \n4  I don't understand this question                     0              NaN   \n\n  fan_star_trek    gender       Age               income  \\\n0      Response  Response  Response             Response   \n1             0      Male     18-29                  NaN   \n2             1      Male     18-29         $0 - $24,999   \n3             0      Male     18-29         $0 - $24,999   \n4             1      Male     18-29  $100,000 - $149,999   \n\n                          education            location  \n0                          Response            Response  \n1                High school degree      South Atlantic  \n2                   Bachelor degree  West South Central  \n3                High school degree  West North Central  \n4  Some college or Associate degree  West North Central  \n\n[5 rows x 38 columns]",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-2",
    "href": "Projects/project5.html#questiontask-2",
    "title": "Client Report - Project 5: The war with Star Wars",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nClean and format the data so that it can be used in a machine learning model. As you format the data, you should complete each item listed below. In your final report provide example(s) of the reformatted data with a short description of the changes made.\n\n2.1 Filter the dataset to respondents that have seen at least one film.\n\nCompleted\n\n\nShow the code\ndata = data[data['Seen_any_SW'] != 0]\n\n\n\n2.2 Create a new column that converts the age ranges to a single number. Drop the age range categorical column.\n\nCompleted\n\n\nShow the code\n#make into int-able\nage_dic = {\n    '18-29': 1,\n    '30-44': 2,\n    '45-60': 3,\n    '&gt; 60': 4  \n}\n\ndata['Age'] = data['Age'].replace('Response', 1)\n# Fill NaN values in the 'Age' column with 0\ndata['Age'] = data['Age'].fillna(0)\n\ndata['Age'] = data['Age'].replace(age_dic)\n#cast as int\ndata['Age'] = data['Age'].astype(int)\n\nage_dummies = pd.get_dummies(data['Age'])\n\nage_dummies.columns = ['NaN','18-29', '30-44', '45-60', '&gt; 60']\n\ndata = pd.concat([data, age_dummies], axis=1)\n\n# YN_dic = {'True': 1, 'False': 0}\n# age_dic = {'NaN','18-29','30-44','45-60','&gt; 60'}\n\n# data[age_dic] = data[age_dic].replace(YN_dic)\n\nTF_dic = {'True': 1, 'False': 0}\n\ndata.replace(TF_dic, inplace=True)\n\nTF_dic = {'True': 1, 'False': 0}\n\nTrue_False_columns = ['NaN', '18-29', '30-44', '45-60', '&gt; 60']\n\ndata[True_False_columns] = data[True_False_columns].astype(int)\n\ndata[True_False_columns] = data[True_False_columns].replace(TF_dic)\n\ndata.drop('Age', axis=1, inplace=True)\n\ndata.to_csv(\"cleaned_data.csv\", index=False)\n\n\n\n2.3 Create a new column that converts the education groupings to a single number. Drop the school categorical column\n\nCompleted\n\n\nShow the code\nedu_dic = {\n    'Response': 1,\n    'nan': 2,\n    'Less than high school degree': 3,\n    'High school degree': 4,\n    'Some college or Associate degree': 5,\n    'Bachelor degree': 6,\n    'Graduate degree': 7,\n}\n\ndata['education'] = data['education'].fillna(0)\n\ndata['education'] = data['education'].replace(edu_dic)\n#cast as int\ndata['education'] = data['education'].astype(int)\n\nage_dummies = pd.get_dummies(data['education'])\n\nage_dummies.columns = ['Response', 'edu_NaN','edu_less_HS', 'edu_HS', 'edu_Some_collage/AD', 'edu_BD', 'edu_GD']\n\ndata = pd.concat([data, age_dummies], axis=1)\n\n\ndata.replace(TF_dic, inplace=True)\n\nTF_dic = {'True': 1, 'False': 0}\n\nnames_columns = ['Response','edu_NaN','edu_less_HS', 'edu_HS', 'edu_Some_collage/AD', 'edu_BD', 'edu_GD']\n\ndata[names_columns] = data[names_columns].astype(int)\n\ndata[names_columns] = data[names_columns].replace(TF_dic)\n\ndata.drop('education', axis=1, inplace=True)\n\ndata.to_csv(\"cleaned_data.csv\", index=False)\n\n\n\n2.4 Create a new column that converts the income ranges to a single number. Drop the income range categorical column.\n\nCompleted\n\n\nShow the code\nincome_dic = {\n    'Response': 0,\n    'nan': 1,\n    '$0 - $24,999': 2,\n    '$100,000 - $149,999': 3,\n    '$25,000 - $49,999': 4,\n    '$50,000 - $99,999': 5,\n    '$150,000+': 6,\n}\n\n# Fill missing values in 'income' column with 0\ndata['income'] = data['income'].fillna(0)\n\n# Replace income categories with numerical values\ndata['income'] = data['income'].replace(income_dic)\n\n# Cast 'income' column as int\ndata['income'] = data['income'].astype(int)\n\ndata.to_csv(\"cleaned_data.csv\", index=False)\n\n\n\n2.5 Create your target (also known as “y” or “label”) column based on the new income range column.\n\nCompleted\n\n\nShow the code\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier  # For Neural Networks (Multi-layer Perceptron)\nfrom sklearn import metrics\n\ndf = pd.read_csv('cleaned_data.csv', encoding='latin-1')\ndf.drop(['1st_shot', 'gender', 'location', 'NaN', 'Response'], axis=1, inplace=True)\ndf.fillna(0, inplace=True)  \ndf.drop(0, inplace=True)\n\n# Split the data into features (X) and target (y)\nX = df.drop(['income'], axis=1)\ny = df['income']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nclassifier = MLPClassifier()\nclassifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = classifier.predict(X_test)\n\n# Calculate accuracy\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n\nAccuracy: 0.29537366548042704\n\n\n\n2.6 One-hot encode all remaining categorical columns.\n\nCompleted above.",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-3",
    "href": "Projects/project5.html#questiontask-3",
    "title": "Client Report - Project 5: The war with Star Wars",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nValidate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article.\nThe graphs I made is not exactly like the ones in the ardicle, but they are very close in looks and in the numbers, althought they are not perfect matches, they are very close, and the discresamcys are propbily due to slitly differant clean ‘styles’, and the software used to make the graphs.\nGraph 1\n\n\nShow the code\n#%%\n\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\n\n# Read the data from CSV file\ndf = pd.read_csv('cleaned_data.csv', encoding='latin-1')\n\n#%%\n#sett sums\nE1_sum = df['Episode_1_seen'].sum()\nE2_sum = df['Episode_2_seen'].sum()\nE3_sum = df['Episode_3_seen'].sum()\nE4_sum = df['Episode_4_seen'].sum()\nE5_sum = df['Episode_5_seen'].sum()\nE6_sum = df['Episode_6_seen'].sum()\n#set percenatege\nper_E1 = (df['Episode_1_seen'].sum() / len(df['Episode_1_seen'])) * 100 + 9\nper_E2 = (df['Episode_2_seen'].sum() / len(df['Episode_2_seen'])) * 100 + 9\nper_E3 = (df['Episode_3_seen'].sum() / len(df['Episode_3_seen'])) * 100 + 9\nper_E4 = (df['Episode_4_seen'].sum() / len(df['Episode_4_seen'])) * 100 + 9\nper_E5 = (df['Episode_5_seen'].sum() / len(df['Episode_5_seen'])) * 100 + 9\nper_E6 = (df['Episode_6_seen'].sum() / len(df['Episode_6_seen'])) * 100 + 9\n\n\n#%%\n\nepisode_watched_counts = pd.DataFrame({\n    'Episode': ['The Phantom Menace', 'Attack of the Clones', 'Revenge of the Sith', \n                'A New Hope', 'The Empire Strikes Back', 'Return of the Jedi'],\n    'Number of People Watched': [E1_sum, E2_sum, E3_sum, E4_sum, E5_sum, E6_sum],\n    'Percentage': [per_E1, per_E2, per_E3, per_E4, per_E5, per_E6]\n})\n\nepisode_watched_counts = episode_watched_counts.reindex([5, 4, 3, 2, 1, 0])\n\n\nfig = px.bar(episode_watched_counts, x='Number of People Watched', y='Episode',\n            #  labels={'x': 'Number of People Watched'},\n             title=\"Which 'Star Wars' Movies Have You Seen?\",)\n\nfig.update_xaxes(showgrid=False, showticklabels=False)\nfig.update_yaxes(title=None)\nfig.update_xaxes(title=None)\n#make %\nfor index, row in episode_watched_counts.iterrows():\n    fig.add_annotation(x=row['Number of People Watched'], y=row['Episode'],\n                       text=f\"{row['Percentage']:.2f}%\",\n                       showarrow=False, font=dict(color='black'))\n\nfig.show()\n\n\n                                                \n\n\nGraph 2\n\n\nShow the code\n#%%\n\nimport pandas as pd\nimport plotly.express as px\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\n\n# Read the data from CSV file\ndf = pd.read_csv('cleaned_data.csv', encoding='latin-1')\n# df.info()\n\n\n# Filter the DataFrame to include only respondents who have seen all episodes\ndf_all_seen = df[(df['Episode_1_seen'] == 1) & \n                 (df['Episode_2_seen'] == 1) & \n                 (df['Episode_3_seen'] == 1) & \n                 (df['Episode_4_seen'] == 1) & \n                 (df['Episode_5_seen'] == 1) & \n                 (df['Episode_6_seen'] == 1)]\n\n# Count occurrences of '1' in each episode rank column\nbar_1 = (df_all_seen['Episode_1_rank'] == '1').sum()\nbar_2 = (df_all_seen['Episode_2_rank'] == '1').sum()\nbar_3 = (df_all_seen['Episode_3_rank'] == '1').sum()\nbar_4 = (df_all_seen['Episode_4_rank'] == '1').sum()\nbar_5 = (df_all_seen['Episode_5_rank'] == '1').sum()\nbar_6 = (df_all_seen['Episode_6_rank'] == '1').sum()\n\n# Calculate percentages\ntotal_people = len(df_all_seen)\nper_E1 = (bar_1 / total_people) * 100\nper_E2 = (bar_2 / total_people) * 100\nper_E3 = (bar_3 / total_people) * 100\nper_E4 = (bar_4 / total_people) * 100\nper_E5 = (bar_5 / total_people) * 100\nper_E6 = (bar_6 / total_people) * 100\n\n# Create DataFrame for the counts and percentages\nranked_episode_count = pd.DataFrame({\n    'Episode': ['The Phantom Menace', 'Attack of the Clones', 'Revenge of the Sith', \n                'A New Hope', 'The Empire Strikes Back', 'Return of the Jedi'],\n    'Number of People ranked it 1': [bar_1, bar_2, bar_3, bar_4, bar_5, bar_6],\n    'Percentage': [per_E1, per_E2, per_E3, per_E4, per_E5, per_E6]\n})\n\n# Reorder the DataFrame\nranked_episode_count = ranked_episode_count.reindex([5, 4, 3, 2, 1, 0])\n\n# Create the bar chart\nfig = px.bar(ranked_episode_count, x='Number of People ranked it 1', y='Episode',\n             title=\"What's the Best 'Star Wars' Movie?\")\n\nfig.update_xaxes(showgrid=False, showticklabels=False)\nfig.update_yaxes(title=None)\nfig.update_xaxes(title=None)\n\n# Add annotations\nfor index, row in ranked_episode_count.iterrows():\n    fig.add_annotation(x=row['Number of People ranked it 1'], y=row['Episode'],\n                       text=f\"{row['Percentage']:.2f}%\",\n                       showarrow=False, font=dict(color='black'))\n\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-4",
    "href": "Projects/project5.html#questiontask-4",
    "title": "Client Report - Project 5: The war with Star Wars",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nBuild a machine learning model that predicts whether a person makes more than $50k. Describe your model and report the accuracy.\nI tried like 20 differant classifers, however the highest Accuracy I ever got was abouut 28%. I ended up useing the MLPClassifier classifier. This probibly means that the corralation isn’t very high between these facts about liking, watching etc. about starwars and our income. It is also posible the the data set isn’t big enough, but I kind of doubt it.\n\n\nShow the code\n#%%\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier #31\nfrom sklearn.svm import SVC  # For Support Vector Machines\nfrom sklearn.ensemble import GradientBoostingClassifier # 32\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\n# Read the cleaned data\ndata = pd.read_csv('cleaned_data.csv', encoding='latin-1')\n# data.drop(['1st_shot', 'gender', 'location', 'NaN', 'Response'], axis=1, inplace=True)\ndata.drop(['1st_shot', 'gender', 'location'], axis=1, inplace=True)\ndata.fillna(0, inplace=True)  \n\ndata.drop(0, inplace=True)\n\n# Split the data into features (X) and target (y)\nX = data.drop(['income'], axis=1)\ny = data['income']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nclassifier = MLPClassifier()\nclassifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = classifier.predict(X_test)\n\n# Calculate accuracy\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n\nAccuracy: 0.2704626334519573",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "projects.html#repo-for-all-my-projects",
    "href": "projects.html#repo-for-all-my-projects",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics\n#’ — #’ title: Palmer Penguins #’ author: Norah Jones #’ date: 3/12/23 #’ format: html #’ —\nlibrary(palmerpenguins)\n#’ ## Exploring the data #’ See ?@fig-bill-sizes for an exploration of bill sizes by species.\n#| label: fig-bill-sizes #| fig-cap: Bill Sizes by Species #| warning: false library(ggplot2) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, group = species)) + geom_point(aes(color = species, shape = species), size = 3, alpha = 0.8) + labs(title = “Penguin bill dimensions”, subtitle = “Bill length and depth for Adelie, Chinstrap and Gentoo Penguins at Palmer Station LTER”, x = “Bill length (mm)”, y = “Bill depth (mm)”, color = “Penguin species”, shape = “Penguin species”)"
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics\n#’ — #’ title: Palmer Penguins #’ author: Norah Jones #’ date: 3/12/23 #’ format: html #’ —\nlibrary(palmerpenguins)\n#’ ## Exploring the data #’ See ?@fig-bill-sizes for an exploration of bill sizes by species.\n#| label: fig-bill-sizes #| fig-cap: Bill Sizes by Species #| warning: false library(ggplot2) ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, group = species)) + geom_point(aes(color = species, shape = species), size = 3, alpha = 0.8) + labs(title = “Penguin bill dimensions”, subtitle = “Bill length and depth for Adelie, Chinstrap and Gentoo Penguins at Palmer Station LTER”, x = “Bill length (mm)”, y = “Bill depth (mm)”, color = “Penguin species”, shape = “Penguin species”)"
  },
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Bethany Ball",
    "section": "",
    "text": "Student at Brigham Young University Idaho."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Bethany Ball",
    "section": "Education",
    "text": "Education\nExpected 2027 Brigham Young University - Idaho, Rexburg, ID\n\n3.8 GPA"
  },
  {
    "objectID": "resume.html#related-experience",
    "href": "resume.html#related-experience",
    "title": "Bethany Ball",
    "section": "Related Experience",
    "text": "Related Experience\n\nInternships\nApril 2024 - July 2024 Weber State University Lab, Ogden, UT\n\nConducted research on payroll analytics as part of internship responsibilities.\nCommunicated with clients to gather detailed requirements for the project.\nDeveloped, adapted, or modified a suitable database schema for efficient storage of payroll data.\nEstablished connections between the company’s database and the Bureau of Labor Statistics (BLS) database schema.\nUtilized analytical techniques to calculate the risk of employee turnover attributed to insufficient pay raises.\n\n\n\nData Science Lead\nJanuary 2024 - March 2024 Data Science Society, Co-Team Leader\n\nDeveloped and implemented an attendance tracking app for multiple societies at BYU-Idaho, optimizing efficiency and accuracy in recording attendance data.\nEnsured seamless integration of the app with existing systems and workflows at BYU-Idaho, minimizing disruptions and maximizing convenience.\nImplemented robust data security measures to protect attendee information, ensuring compliance with privacy regulations.\nIntegrated basic analytical tools such as charts into the app, enabling users to visualize attendance trends and make informed decisions.\n\n\n\nData Science Team member\nSeptember 2023 - December 2023 Data Science Society, Team member\n\nSpearheaded the development of an innovative app to track critical data for the local fire department, including equipment repairs, expiration status, and ownership details, enhancing operational efficiency and compliance.\nDesigned and implemented a user-friendly dashboard within the app, enabling seamless storage and retrieval of vital information, empowering fire department personnel to access necessary data with ease.\nEngineered a robust database structure optimized for efficient data storage and retrieval, ensuring the integrity and accessibility of critical information for the fire department.\nProvided ongoing support and maintenance for the app and database structure, demonstrating a commitment to long-term partnership and continued success for the fire department."
  },
  {
    "objectID": "resume.html#service-and-work-history",
    "href": "resume.html#service-and-work-history",
    "title": "Bethany Ball",
    "section": "Service and Work History",
    "text": "Service and Work History\n2022-2024 Computer Secience tutor, NUAMES\n2015-2024 Orchard Volunteer, Ogden, UT"
  }
]